{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 178\n",
      "1 182\n",
      "2 177\n",
      "3 183\n",
      "4 181\n",
      "5 182\n",
      "6 181\n",
      "7 179\n",
      "8 174\n",
      "9 180\n"
     ]
    }
   ],
   "source": [
    "for class_name, class_count in zip(dataset.target_names, np.bincount(dataset.target)):\n",
    "    print(class_name,class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (1797, 64)\n",
      "\n",
      "Number of Classes: 10\n",
      "\n",
      "Feature Names: Each feature represents pixel intensity values\n",
      "\n",
      "Samples per digit:\n",
      "target\n",
      "0    178\n",
      "1    182\n",
      "2    177\n",
      "3    183\n",
      "4    181\n",
      "5    182\n",
      "6    181\n",
      "7    179\n",
      "8    174\n",
      "9    180\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"Dataset Shape:\", digits.data.shape)\n",
    "print(\"\\nNumber of Classes:\", len(digits.target_names))\n",
    "print(\"\\nFeature Names: Each feature represents pixel intensity values\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "digits_df = pd.DataFrame(digits.data)\n",
    "digits_df['target'] = digits.target\n",
    "\n",
    "# Count samples per class\n",
    "print(\"\\nSamples per digit:\")\n",
    "print(digits_df['target'].value_counts().sort_index())\n",
    "\n",
    "# Visualize one example\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(digits.images[0], cmap='gray')\n",
    "plt.title(f\"Example Image: Digit {digits.target[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels:\t [1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n",
      "New binary labels:\t [1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset with imbalanced binary classes:  \n",
    "# Negative class (0) is 'not digit 1' \n",
    "# Positive class (1) is 'digit 1'\n",
    "y_binary_imbalanced = y.copy()\n",
    "y_binary_imbalanced[y_binary_imbalanced != 1] = 0\n",
    "\n",
    "print('Original labels:\\t', y[1:30])\n",
    "print('New binary labels:\\t', y_binary_imbalanced[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1615,  182])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_binary_imbalanced)    # Negative class (0) is the most frequent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation score: 0.991\n",
      "best n_neighbors: 1\n",
      "test-set score: 0.982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=42)\n",
    "\n",
    "val_scores = []\n",
    "neighbors = np.arange(1, 15, 1)\n",
    "for i in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    val_scores.append(knn.score(X_val, y_val))\n",
    "\n",
    "print(\"best validation score: {:.3f}\".format(np.max(val_scores)))\n",
    "best_n_neighbors = neighbors[np.argmax(val_scores)]\n",
    "print(\"best n_neighbors:\", best_n_neighbors)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
    "knn.fit(X_trainval, y_trainval)\n",
    "print(\"test-set score: {:.3f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation score: 0.843\n",
      "best max_depth: 10\n",
      "test-set score: 0.867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Split data into train+validation and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=42)\n",
    "# Split train+validation into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=42)\n",
    "\n",
    "# Create lists to store validation scores\n",
    "val_scores = []\n",
    "# Test different depths from 1 to 15\n",
    "depths = np.arange(1, 15, 1)\n",
    "\n",
    "# Try each depth and record validation scores\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    val_scores.append(tree.score(X_val, y_val))\n",
    "\n",
    "# Find and print the best validation score and corresponding depth\n",
    "print(\"best validation score: {:.3f}\".format(np.max(val_scores)))\n",
    "best_depth = depths[np.argmax(val_scores)]\n",
    "print(\"best max_depth:\", best_depth)\n",
    "\n",
    "# Train final model with best depth on combined training and validation data\n",
    "tree = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\n",
    "tree.fit(X_trainval, y_trainval)\n",
    "print(\"test-set score: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation score: 0.843\n",
      "Best parameters:\n",
      "max_depth: 10\n",
      "min_samples_leaf: 1\n",
      "Test-set score: 0.867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Split data into train+validation and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=42)\n",
    "# Split train+validation into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=42)\n",
    "\n",
    "# Define parameter ranges\n",
    "depths = np.arange(1, 15, 1)\n",
    "min_samples = np.logspace(0, 3, 10).astype(int)  # Creates array from 1 to 1000\n",
    "\n",
    "# Create variables to store best parameters and scores\n",
    "best_score = 0\n",
    "best_depth = None\n",
    "best_min_samples = None\n",
    "val_scores = []\n",
    "\n",
    "# Nested loops to try all combinations\n",
    "for depth in depths:\n",
    "    for min_leaf in min_samples:\n",
    "        tree = DecisionTreeClassifier(\n",
    "            max_depth=depth,\n",
    "            min_samples_leaf=min_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "        tree.fit(X_train, y_train)\n",
    "        score = tree.score(X_val, y_val)\n",
    "        val_scores.append({\n",
    "            'max_depth': depth,\n",
    "            'min_samples_leaf': min_leaf,\n",
    "            'score': score\n",
    "        })\n",
    "        \n",
    "        # Update best parameters if we find a better score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_depth = depth\n",
    "            best_min_samples = min_leaf\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best validation score: {:.3f}\".format(best_score))\n",
    "print(\"Best parameters:\")\n",
    "print(\"max_depth:\", best_depth)\n",
    "print(\"min_samples_leaf:\", best_min_samples)\n",
    "\n",
    "# Train final model with best parameters on combined training and validation data\n",
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=best_depth,\n",
    "    min_samples_leaf=best_min_samples,\n",
    "    random_state=42\n",
    ")\n",
    "tree.fit(X_trainval, y_trainval)\n",
    "print(\"Test-set score: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 140 candidates, totalling 700 fits\n",
      "Best parameters from GridSearchCV:\n",
      "{'max_depth': 11, 'min_samples_leaf': 1}\n",
      "Best cross-validation score: 0.842\n",
      "Test-set score: 0.871\n",
      "Test-set score (directly with grid object): 0.871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': depths,\n",
    "    'min_samples_leaf': min_samples\n",
    "}\n",
    "# Verbose parameter only indicates the progress of the search, more verbose means more output\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, verbose=1)\n",
    "\n",
    "grid_search.fit(X_trainval, y_trainval)\n",
    "print(\"Best parameters from GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Train final model with best parameters on combined training and validation data\n",
    "tree = grid_search.best_estimator_\n",
    "tree.fit(X_trainval, y_trainval)\n",
    "print(\"Test-set score: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "# Is possible to get the score directly from the grid object\n",
    "print(\"Test-set score (directly with grid object): {:.3f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "max_depth: 11\n",
      "min_samples_leaf: 1\n",
      "\n",
      "Best cross-validation score: 0.842\n",
      "Test set score: 0.871\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "\n",
      "Rank 1:\n",
      "max_depth: 11\n",
      "min_samples_leaf: 1\n",
      "Mean CV Score: 0.842 (+/- 0.042)\n",
      "\n",
      "Rank 2:\n",
      "max_depth: 10\n",
      "min_samples_leaf: 1\n",
      "Mean CV Score: 0.841 (+/- 0.027)\n",
      "\n",
      "Rank 3:\n",
      "max_depth: 12\n",
      "min_samples_leaf: 1\n",
      "Mean CV Score: 0.840 (+/- 0.043)\n",
      "\n",
      "Rank 4:\n",
      "max_depth: 9\n",
      "min_samples_leaf: 1\n",
      "Mean CV Score: 0.839 (+/- 0.052)\n",
      "\n",
      "Rank 5:\n",
      "max_depth: 13\n",
      "min_samples_leaf: 1\n",
      "Mean CV Score: 0.838 (+/- 0.040)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': np.arange(1, 15, 1),\n",
    "    'min_samples_leaf': np.logspace(0, 3, 10).astype(int)\n",
    "}\n",
    "\n",
    "# Create and fit GridSearchCV\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters:\")\n",
    "print(f\"max_depth: {grid_search.best_params_['max_depth']}\")\n",
    "print(f\"min_samples_leaf: {grid_search.best_params_['min_samples_leaf']}\")\n",
    "print(f\"\\nBest cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Test set score: {grid_search.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Create DataFrame of all results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "# Sort by mean test score in descending order\n",
    "top_results = results.sort_values('mean_test_score', ascending=False).head()\n",
    "\n",
    "print(\"\\nTop 5 parameter combinations:\")\n",
    "for idx, row in top_results.iterrows():\n",
    "    print(f\"\\nRank {row['rank_test_score']}:\")\n",
    "    print(f\"max_depth: {row['param_max_depth']}\")\n",
    "    print(f\"min_samples_leaf: {row['param_min_samples_leaf']}\")\n",
    "    print(f\"Mean CV Score: {row['mean_test_score']:.3f} (+/- {row['std_test_score']*2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.987\n",
      "best parameters: {'n_neighbors': 1}\n",
      "test-set score: 0.982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters:\", grid.best_params_)\n",
    "\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
       "       'param_n_neighbors', 'params', 'split0_test_score', 'split1_test_score',\n",
       "       'split2_test_score', 'split3_test_score', 'split4_test_score',\n",
       "       'split5_test_score', 'split6_test_score', 'split7_test_score',\n",
       "       'split8_test_score', 'split9_test_score', 'mean_test_score',\n",
       "       'std_test_score', 'rank_test_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {'n_neighbors': 1}\n",
       "1     {'n_neighbors': 3}\n",
       "2     {'n_neighbors': 5}\n",
       "3     {'n_neighbors': 7}\n",
       "4     {'n_neighbors': 9}\n",
       "5    {'n_neighbors': 11}\n",
       "6    {'n_neighbors': 13}\n",
       "Name: params, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       0.001135      0.000282         0.002101        0.000376   \n",
      "1       0.001000      0.000186         0.001927        0.000186   \n",
      "2       0.000906      0.000092         0.002079        0.000379   \n",
      "3       0.000905      0.000060         0.002251        0.000149   \n",
      "4       0.000892      0.000070         0.002283        0.000110   \n",
      "5       0.000955      0.000083         0.002458        0.000118   \n",
      "6       0.000952      0.000125         0.002503        0.000120   \n",
      "\n",
      "   param_n_neighbors               params  split0_test_score  \\\n",
      "0                  1   {'n_neighbors': 1}           0.985185   \n",
      "1                  3   {'n_neighbors': 3}           0.992593   \n",
      "2                  5   {'n_neighbors': 5}           0.992593   \n",
      "3                  7   {'n_neighbors': 7}           0.985185   \n",
      "4                  9   {'n_neighbors': 9}           0.977778   \n",
      "5                 11  {'n_neighbors': 11}           0.970370   \n",
      "6                 13  {'n_neighbors': 13}           0.970370   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0           0.992593           0.977778           0.992593           0.970370   \n",
      "1           0.985185           0.970370           0.992593           0.955556   \n",
      "2           0.992593           0.970370           0.992593           0.970370   \n",
      "3           0.985185           0.977778           0.985185           0.962963   \n",
      "4           0.977778           0.977778           0.985185           0.955556   \n",
      "5           0.985185           0.970370           0.985185           0.940741   \n",
      "6           0.985185           0.940741           0.977778           0.948148   \n",
      "\n",
      "   split5_test_score  split6_test_score  split7_test_score  split8_test_score  \\\n",
      "0           1.000000           0.992593           0.992537           0.992537   \n",
      "1           0.992593           1.000000           0.992537           0.977612   \n",
      "2           0.992593           0.992593           0.985075           0.977612   \n",
      "3           0.992593           0.977778           0.992537           0.962687   \n",
      "4           0.992593           0.985185           0.992537           0.962687   \n",
      "5           0.992593           0.985185           0.992537           0.962687   \n",
      "6           0.992593           0.977778           0.985075           0.962687   \n",
      "\n",
      "   split9_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0           0.977612         0.987380        0.008808                1  \n",
      "1           0.977612         0.983665        0.012756                3  \n",
      "2           0.985075         0.985146        0.008779                2  \n",
      "3           0.970149         0.979204        0.010426                4  \n",
      "4           0.970149         0.977722        0.011519                5  \n",
      "5           0.970149         0.975500        0.015223                6  \n",
      "6           0.970149         0.971050        0.015702                7  \n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificadores Base (Dummy Classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 178\n",
      "1 182\n",
      "2 177\n",
      "3 183\n",
      "4 181\n",
      "5 182\n",
      "6 181\n",
      "7 179\n",
      "8 174\n",
      "9 180\n"
     ]
    }
   ],
   "source": [
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Vemos la distribuci√≥n de clases, apreciendo que es balanceada\n",
    "for class_name, class_count in zip(dataset.target_names, np.bincount(dataset.target)):\n",
    "    print(class_name,class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels:\t [1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n",
      "New binary labels:\t [1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset with imbalanced binary classes:  \n",
    "# Negative class (0) is 'not digit 1' \n",
    "# Positive class (1) is 'digit 1'\n",
    "y_binary_imbalanced = y.copy()\n",
    "y_binary_imbalanced[y_binary_imbalanced != 1] = 0\n",
    "\n",
    "# Con esta transformaci√≥n, la clase negativa (0) es la m√°s frecuente, ahora est√° desbalanceado el dataset\n",
    "print('Original labels:\\t', y[1:30])\n",
    "print('New binary labels:\\t', y_binary_imbalanced[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, stratify = y_binary_imbalanced, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Negative class (0) is most frequent\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "# Therefore the dummy 'most_frequent' classifier always predicts class 0\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "\n",
    "y_dummy_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8977777777777778"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_majority.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mismo dataset con KNN obten√≠a tambi√©n un score superior al 90%, esto es se√±al de que esta m√©trica no es la mejor posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binaria para 2 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent class (dummy classifier)\n",
      " [[404   0]\n",
      " [ 46   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Negative class (0) is most frequent\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "y_majority_predicted = dummy_majority.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_majority_predicted)\n",
    "\n",
    "print('Most frequent class (dummy classifier)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random class-proportional prediction (dummy classifier)\n",
      " [[370  34]\n",
      " [ 43   3]]\n"
     ]
    }
   ],
   "source": [
    "# produces random predictions w/ same class proportion as training set\n",
    "dummy_classprop = DummyClassifier(strategy='stratified').fit(X_train, y_train)\n",
    "y_classprop_predicted = dummy_classprop.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_classprop_predicted)\n",
    "\n",
    "print('Random class-proportional prediction (dummy classifier)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier (default settings)\n",
      " [[399   5]\n",
      " [  4  42]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "lr_predicted = lr.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, lr_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree classifier (max_depth = 2)\n",
      " [[393  11]\n",
      " [ 20  26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "tree_predicted = dt.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, tree_predicted)\n",
    "\n",
    "print('Decision tree classifier (max_depth = 2)\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n de M√©tricas para Clasificaci√≥n Binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Precision: 0.70\n",
      "Recall: 0.57\n",
      "F1: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) \n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)))\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted)))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, tree_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       404\n",
      "           1       0.70      0.57      0.63        46\n",
      "\n",
      "    accuracy                           0.93       450\n",
      "   macro avg       0.83      0.77      0.79       450\n",
      "weighted avg       0.93      0.93      0.93       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined report with all above metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, tree_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       not 1       0.95      0.97      0.96       404\n",
      "           1       0.70      0.57      0.63        46\n",
      "\n",
      "    accuracy                           0.93       450\n",
      "   macro avg       0.83      0.77      0.79       450\n",
      "weighted avg       0.93      0.93      0.93       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined report with all above metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, tree_predicted,  target_names=['not 1', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'not 1': {'precision': 0.9515738498789347,\n",
       "  'recall': 0.9727722772277227,\n",
       "  'f1-score': 0.9620563035495716,\n",
       "  'support': 404.0},\n",
       " '1': {'precision': 0.7027027027027027,\n",
       "  'recall': 0.5652173913043478,\n",
       "  'f1-score': 0.6265060240963856,\n",
       "  'support': 46.0},\n",
       " 'accuracy': 0.9311111111111111,\n",
       " 'macro avg': {'precision': 0.8271382762908187,\n",
       "  'recall': 0.7689948342660353,\n",
       "  'f1-score': 0.7942811638229785,\n",
       "  'support': 450.0},\n",
       " 'weighted avg': {'precision': 0.9261336881675866,\n",
       "  'recall': 0.9311111111111111,\n",
       "  'f1-score': 0.9277556083165792,\n",
       "  'support': 450.0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, tree_predicted,  target_names=['not 1', '1'], output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizaci√≥n de Par√°metros Especificando M√©trica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.965\n",
      "best parameters: {'n_neighbors': 5}\n",
      "test-set score: 0.979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, stratify = y_binary_imbalanced, random_state=0)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10, scoring='precision')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters:\", grid.best_params_)\n",
    "\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 1.000\n",
      "best parameters: {'n_neighbors': 1}\n",
      "test-set score: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, stratify = y_binary_imbalanced, random_state=0)\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1, 15, 2)}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10, scoring='recall')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters:\", grid.best_params_)\n",
    "\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
